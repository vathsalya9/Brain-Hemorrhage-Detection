{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_2_sample_submission.csv  stage_2_test  stage_2_train  stage_2_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '/data/cmpe257-02-fa2019/team-1-meerkats/rsna-intracranial-hemorrhage-detection/'\n",
    "!ls $DATA_DIR\n",
    "INPUT_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# dcm processing\n",
    "\n",
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):\n",
    "    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    \n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "\n",
    "\n",
    "def read_trainset(filename=DATA_DIR+\"stage_2_train.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    \n",
    "    duplicates_to_remove = [\n",
    "        56346,56347,56348,56349,\n",
    "        56350,56351,1171830,1171831,\n",
    "        1171832,1171833,1171834,1171835,\n",
    "        3705312,3705313,3705314,3705315,\n",
    "        3705316,3705317,3842478,3842479,\n",
    "        3842480,3842481,3842482,3842483\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(index=duplicates_to_remove)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import cv2\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "np.random.seed(257)\n",
    "\n",
    "def _read(path, desired_size):\n",
    "    \"\"\"Will be used in DataGenerator\"\"\"\n",
    "    \n",
    "    dcm = pydicom.dcmread(path)\n",
    "    \n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "        img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    # Some dcms seem to be corrupted\n",
    "    except ValueError:\n",
    "        print('Error while parsing {}'.format(path))\n",
    "        img = np.ones(desired_size)\n",
    "    \n",
    "    return img\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, img_dir, image_IDs, labels_df, batch_size, img_size):\n",
    "\n",
    "        self.image_IDs = image_IDs\n",
    "        self.labels_df = labels_df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.image_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_ids = self.image_IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        Y = np.empty((self.batch_size, 6))\n",
    "        \n",
    "        for i, ID in enumerate(batch_ids):\n",
    "            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            Y[i,] = self.labels_df.loc[ID].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit \n",
    "\n",
    "train_df = read_trainset()\n",
    "\n",
    "# k-fold splitting\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=257).split(train_df.index)\n",
    "# get indeces for one split\n",
    "train_idx, valid_idx = next(ss)\n",
    "train_df_kfold = train_df.iloc[train_idx]\n",
    "valid_df_kfold = train_df.iloc[valid_idx]\n",
    "\n",
    "\n",
    "traingen = DataGenerator(img_dir=DATA_DIR+'stage_2_train/',\n",
    "                         image_IDs=train_df_kfold.index, #MAGIC\n",
    "                         labels_df=train_df_kfold, #MAGIC\n",
    "                         batch_size=16,\n",
    "                         img_size=INPUT_SHAPE)\n",
    "\n",
    "validgen = DataGenerator(img_dir=DATA_DIR+'stage_2_train/',\n",
    "                         image_IDs=valid_df_kfold.index, #MAGIC\n",
    "                         labels_df=valid_df_kfold, #MAGIC\n",
    "                         batch_size=16,\n",
    "                         img_size=INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function\n",
    "from keras import backend as K\n",
    "\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Can be used as the loss function in model.compile()\n",
    "    ---------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    out = -(        3* y_true  * K.log(      y_pred) * class_weights\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "    return K.mean(out, axis=-1)\n",
    "\n",
    "# custom performance metric\n",
    "def correct_diagnoses(y_true, y_pred):\n",
    "    THRESHOLD = 0.5\n",
    "    p_thr = K.greater(y_pred, THRESHOLD)\n",
    "    y_true = K.cast(y_true, dtype='bool')\n",
    "    \n",
    "    equals_t = K.equal(p_thr, y_true)\n",
    "    correct_rows = K.all(equals_t, axis=1)\n",
    "    correct_rows_int = K.cast(correct_rows, dtype='int32')\n",
    "    \n",
    "    return K.sum(correct_rows_int)/K.shape(correct_rows_int)[0]\n",
    "\n",
    "def correct_positive_diagnoses(y_true, y_pred):\n",
    "    THRESHOLD = 0.5\n",
    "    p_thr = K.greater(y_pred, THRESHOLD)\n",
    "    y_true = K.cast(y_true, dtype='bool')\n",
    "    \n",
    "    pos_mask = K.any(y_true, axis=1) #patients with positive diagnoses\n",
    "    p_thr = p_thr[pos_mask]\n",
    "    y_true = y_true[pos_mask]\n",
    "    \n",
    "    equals_t = K.equal(p_thr, y_true)\n",
    "    correct_rows = K.all(equals_t, axis=1)\n",
    "    correct_rows_float = K.cast(correct_rows, dtype='float32')\n",
    "    \n",
    "    return K.sum(correct_rows_float)/(K.cast(K.shape(correct_rows_float)[0], dtype='float32')+K.epsilon())\n",
    "\n",
    "def np_cpd(y_true, pred, thr=0.5): #numpy implementation of correct positive diagnoses\n",
    "    p_thr = pred > thr\n",
    "\n",
    "    pos_mask = np.any(y_true, axis=1)\n",
    "\n",
    "    p_thr = p_thr[pos_mask]\n",
    "    y_true = y_true[pos_mask]\n",
    "\n",
    "    p_correct = np.all(p_thr[:len(y_true)] == y_true[:len(p_thr)], axis=1)\n",
    "\n",
    "    return np.sum(p_correct)/(len(p_thr)+1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 21,139,014\n",
      "Trainable params: 6,424,326\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "conv_base = VGG16(weights=None, input_shape=INPUT_SHAPE ,include_top=False)\n",
    "conv_base.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5') #doesn't work otherwise without internet access\n",
    "\n",
    "conv_base.trainable = False\n",
    "model = keras.models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "model.name = 'VGG_frozen_base'\n",
    "model.compile(\n",
    "    loss=weighted_log_loss, #custom loss\n",
    "    #loss='binary_crossentropy',\n",
    "    #loss='categorical_crossentropy', # mutually exclusive\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "    metrics=[correct_positive_diagnoses])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "mc = keras.callbacks.ModelCheckpoint(filepath='weights/VGG_frozen_base-epoch={epoch:02d}-valid-loss={val_loss:.2f}.hdf5', monitor='loss', verbose=True, save_best_only=False, save_weights_only=False)\n",
    "tb = keras.callbacks.TensorBoard(log_dir=f'./Graph-{int(time.time())}', histogram_freq=0, update_freq=6700,\n",
    "          write_graph=True, write_images=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1250/42346 [..............................] - ETA: 5:27:43 - loss: 0.4984 - correct_positive_diagnoses: 0.0602"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(traingen,\n",
    "                    validation_data = validgen,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=28,\n",
    "                    callbacks=[mc, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Not learning. I suspect it to be either:\n",
    "- problem with the loss function\n",
    "- perhaps the dense head is out of its capacity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-brainenv",
   "language": "python",
   "name": "brainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
