{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TRAIN_DIR = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n",
    "#TEST_DIR = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/' #dont't touch test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 4s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              51201000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               250250    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 73,255,540\n",
      "Trainable params: 73,221,108\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import layers\n",
    "\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "conv_base = InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=INPUT_SHAPE, pooling=None, classes=1000)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dense(250, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    "    \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "def _read(path, desired_size):\n",
    "    \"\"\"Will be used in DataGenerator\"\"\"\n",
    "    \n",
    "    dcm = pydicom.dcmread(path)\n",
    "    \n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(desired_size)\n",
    "    \n",
    "    \n",
    "    img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, img_dir, id_list, labels, batch_size, img_size):\n",
    "\n",
    "        self.list_IDs = id_list\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            X, Y = self.__data_generation(list_IDs_temp)\n",
    "            return X, Y\n",
    "        else:\n",
    "            X = self.__data_generation(list_IDs_temp)\n",
    "            return X\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        \n",
    "        if self.labels is not None: # for training phase we undersample and shuffle\n",
    "            # keep probability of any=0 and any=1\n",
    "            keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "            keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "            self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "            np.random.shuffle(self.indices)\n",
    "        else:\n",
    "            self.indices = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        if self.labels is not None: # training phase\n",
    "            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                X[i] = np.dstack(3*[_read(self.img_dir+ID+\".dcm\", self.img_size)[...,-1]])\n",
    "                Y[i] = self.labels.loc[ID].values\n",
    "        \n",
    "            return X, Y\n",
    "        \n",
    "        else: # test phase\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                X[i] = np.dstack(3*[_read(self.img_dir+ID+\".dcm\", self.img_size)[...,-1]])\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Credit: https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model\n",
    "def read_testset(filename=\"../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_trainset(filename=\"../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    \n",
    "    duplicates_to_remove = [\n",
    "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
    "        312468,  312469,  312470,  312471,  312472,  312473,\n",
    "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
    "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(index=duplicates_to_remove)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit \n",
    "\n",
    "train_df = read_trainset()\n",
    "\n",
    "# k-fold splitting\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=257).split(train_df.index)\n",
    "# get indeces for one split\n",
    "train_idx, valid_idx = next(ss)\n",
    "train_df_kfold = train_df.iloc[train_idx]\n",
    "valid_df_kfold = train_df.iloc[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicions = None\n",
    "class PredictionCheckpoint(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_df, validation_dir=TRAIN_DIR, \n",
    "                 batch_size=32, input_size=INPUT_SHAPE):\n",
    "        \n",
    "        self.valid_df = validation_dir\n",
    "        self.valid_images_dir = validation_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.validation_predictions = []\n",
    "        self.accu = []\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.validation_predictions = []\n",
    "        self.accu = []\n",
    "        \n",
    "    def on_epoch_end(self,batch, logs={}):\n",
    "        #check how we're doing on validation\n",
    "        validgen = DataGenerator(img_dir=TRAIN_DIR, id_list=self.valid_df.index, labels=None, batch_size=self.batch_size, img_size=self.input_size)\n",
    "        global predictions\n",
    "        predictions = self.model.predict_generator(validgen, verbose=2)\n",
    "        valid_len = len(valid_df_kfold.to_numpy())\n",
    "        acc = sum(sum(np.equal(predictions[:valid_len], valid_df_kfold.to_numpy())))/(predictions.shape[0]*predictions.shape[1]) \n",
    "        print(\"Tom's Metic\", acc)\n",
    "        self.accu.append(acc)\n",
    "        self.validation_predictions.append(predictions)        \n",
    "        \n",
    "pred_history = PredictionCheckpoint(valid_df_kfold, valid_df_kfold, input_size=INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGenerator(img_dir=TRAIN_DIR,\n",
    "                    id_list=train_df_kfold.index,\n",
    "                    labels=train_df_kfold,\n",
    "                    batch_size=32,\n",
    "                    img_size=INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7046/7046 [==============================] - 2410s 342ms/step - loss: 0.8166 - accuracy: 0.9999\n",
      "Tom's Metic 0.22180878399746995\n",
      "Epoch 2/5\n",
      "7034/7034 [==============================] - 2379s 338ms/step - loss: 0.8265 - accuracy: 1.0000\n",
      "Tom's Metic 0.22180878399746995\n",
      "Epoch 3/5\n",
      "4845/7048 [===================>..........] - ETA: 12:27 - loss: 0.8264 - accuracy: 1.0000Tom's Metic 0.22180878399746995\n",
      "Epoch 4/5\n",
      "7052/7052 [==============================] - 2402s 341ms/step - loss: 0.8257 - accuracy: 1.0000\n",
      "Tom's Metic 0.22180878399746995\n",
      "Epoch 5/5\n",
      "7030/7030 [==============================] - 2410s 343ms/step - loss: 0.8252 - accuracy: 1.0000\n",
      "Tom's Metic 0.22180878399746995\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(gen,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    callbacks=[pred_history])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
