{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\n\nDATA_DIR = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\n!ls $DATA_DIR\nINPUT_SHAPE = (224, 224, 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# dcm processing\n\ndef correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):\n    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    \n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n\n    return bsb_img\n\n\ndef read_trainset(filename=DATA_DIR+\"stage_2_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    \n    duplicates_to_remove = [\n        56346,56347,56348,56349,\n        56350,56351,1171830,1171831,\n        1171832,1171833,1171834,1171835,\n        3705312,3705313,3705314,3705315,\n        3705316,3705317,3842478,3842479,\n        3842480,3842481,3842482,3842483\n    ]\n    \n    df = df.drop(index=duplicates_to_remove)\n    df = df.reset_index(drop=True)\n    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport cv2\nfrom math import ceil\nimport numpy as np\nimport pydicom\n\nnp.random.seed(257)\n\ndef _read(path, desired_size):\n    \"\"\"Will be used in DataGenerator\"\"\"\n    \n    dcm = pydicom.dcmread(path)\n    \n    try:\n        img = bsb_window(dcm)\n        img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n        \n    # Some dcms seem to be corrupted\n    except ValueError:\n        print('Error while parsing {}'.format(path))\n        img = np.ones(desired_size)\n    \n    return img\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, img_dir, image_IDs, labels_df, batch_size, img_size):\n\n        self.image_IDs = image_IDs\n        self.labels_df = labels_df\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n\n    def __len__(self):\n        return int(ceil(len(self.image_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        \n        batch_ids = self.image_IDs[index*self.batch_size:(index+1)*self.batch_size]\n        \n        X = np.empty((self.batch_size, *self.img_size))\n        Y = np.empty((self.batch_size, 6))\n        \n        for i, ID in enumerate(batch_ids):\n            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            Y[i,] = self.labels_df.loc[ID].values\n        \n        return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit \n\ntrain_df = read_trainset()\n\n# k-fold splitting\nss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=257).split(train_df.index)\n# get indeces for one split\ntrain_idx, valid_idx = next(ss)\ntrain_df_kfold = train_df.iloc[train_idx]\nvalid_df_kfold = train_df.iloc[valid_idx]\n\n\ntraingen = DataGenerator(img_dir=DATA_DIR+'stage_2_train/',\n                         image_IDs=train_df_kfold.index[:10000], #MAGIC\n                         labels_df=train_df_kfold[:10000], #MAGIC\n                         batch_size=16,\n                         img_size=INPUT_SHAPE)\n\nvalidgen = DataGenerator(img_dir=DATA_DIR+'stage_2_train/',\n                         image_IDs=valid_df_kfold.index[:1000], #MAGIC\n                         labels_df=valid_df_kfold[:1000], #MAGIC\n                         batch_size=16,\n                         img_size=INPUT_SHAPE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom loss function\nfrom keras import backend as K\n\ndef weighted_log_loss(y_true, y_pred):\n    \"\"\"\n    Can be used as the loss function in model.compile()\n    ---------------------------------------------------\n    \"\"\"\n    \n    class_weights = np.array([1., 1., 1., 1., 1., 1.])\n    \n    eps = K.epsilon()\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    out = -(         3 * y_true  * K.log(      y_pred) * class_weights\n            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n    \n    return K.mean(out, axis=-1)\n\n# custom performance metric\ndef correct_diagnoses(y_true, y_pred):\n    THRESHOLD = 0.5\n    p_thr = K.greater(y_pred, THRESHOLD)\n    y_true = K.cast(y_true, dtype='bool')\n    \n    equals_t = K.equal(p_thr, y_true)\n    correct_rows = K.all(equals_t, axis=1)\n    correct_rows_int = K.cast(correct_rows, dtype='int32')\n    \n    return K.sum(correct_rows_int)/K.shape(correct_rows_int)[0]\n\ndef correct_positive_diagnoses(y_true, y_pred):\n    THRESHOLD = 0.5\n    p_thr = K.greater(y_pred, THRESHOLD)\n    y_true = K.cast(y_true, dtype='bool')\n    \n    pos_mask = K.any(y_true, axis=1) #patients with positive diagnoses\n    p_thr = p_thr[pos_mask]\n    y_true = y_true[pos_mask]\n    \n    equals_t = K.equal(p_thr, y_true)\n    correct_rows = K.all(equals_t, axis=1)\n    correct_rows_float = K.cast(correct_rows, dtype='float32')\n    \n    return K.sum(correct_rows_float)/(K.cast(K.shape(correct_rows_float)[0], dtype='float32')+K.epsilon())\n\ndef np_cpd(y_true, pred, thr=0.5): #numpy implementation of correct positive diagnoses\n    p_thr = pred > thr\n\n    pos_mask = np.any(y_true, axis=1)\n\n    p_thr = p_thr[pos_mask]\n    y_true = y_true[pos_mask]\n\n    p_correct = np.all(p_thr[:len(y_true)] == y_true[:len(p_thr)], axis=1)\n\n    return np.sum(p_correct)/(len(p_thr)+1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.applications.vgg16 import VGG16\nfrom keras import layers\nimport numpy as np\n\nconv_base = VGG16(weights='imagenet', input_shape=INPUT_SHAPE ,include_top=False)\n\nconv_base.trainable = False\nmodel = keras.models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(6, activation='sigmoid'))\n\nmodel.compile(\n    loss=weighted_log_loss, #custom loss\n    #loss='binary_crossentropy',\n    #loss='categorical_crossentropy', # mutually exclusive\n    optimizer=keras.optimizers.Adam(lr=1e-3),\n    metrics=[correct_positive_diagnoses])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator(traingen,\n                    epochs=20,\n                    verbose=True,\n                    use_multiprocessing=True,\n                    workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = model.predict_generator(validgen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model_20epochs_10k.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in np.linspace(0.1,1.0,10):\n    print(\"THR {} cpd: {}\".format(t, np_cpd(valid_df_kfold[:1000].to_numpy(), p[:1000], thr=t))) # validation accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the validation results are obviously poor (We're overfitting on )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_tr = model.predict_generator(traingen)\nfor t in np.linspace(0.1,1.0,10):\n    print(\"THR {} cpd: {}\".format(t, np_cpd(train_df_kfold[:1000].to_numpy(), p_tr[:1000], thr=t))) # validation accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sanity check\nimport matplotlib.pyplot as plt\nx,y = traingen[0]\nplt.imshow(x[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_tr > 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.any(train_df_kfold[:1000], axis=1) # positive examples\np_tr[:1000][mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(p_tr[:1000][mask],1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nSome good results. I think that the loss function is the bottleneck at this point."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}